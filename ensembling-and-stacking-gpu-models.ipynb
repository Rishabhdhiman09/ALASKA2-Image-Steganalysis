{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from collections import OrderedDict, namedtuple\n",
    "from torch.optim import lr_scheduler\n",
    "from glob import glob  # find all pathnames matching certain patterns\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2  # for converting image to tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler  # return indices in sequence/random order\n",
    "import cv2\n",
    "from sklearn.metrics import *\n",
    "from tqdm.notebook import tqdm\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Seed everything for reproducable results\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\r\n",
      "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.5.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (1.18.1)\r\n",
      "Building wheels for collected packages: efficientnet-pytorch\r\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=dfe42f3dc885726c5269864375bb659a547aa3b6d8f63aceab4cf598ec4d533f\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\r\n",
      "Successfully built efficientnet-pytorch\r\n",
      "Installing collected packages: efficientnet-pytorch\r\n",
      "Successfully installed efficientnet-pytorch-0.6.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet_pytorch\n",
    "# import efficientnet_pytorch\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_net0():\n",
    "#     net = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "#     net._fc = nn.Linear(in_features=1280, out_features=4, bias=True)\n",
    "#     return net\n",
    "\n",
    "# mx0 = get_net0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b2-8bb594d6.pth\" to /root/.cache/torch/checkpoints/efficientnet-b2-8bb594d6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c0a8d7fa6646fa84a52ab9e85d5584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36804509.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded pretrained weights for efficientnet-b2\n"
     ]
    }
   ],
   "source": [
    "def get_net2():\n",
    "    net = EfficientNet.from_pretrained('efficientnet-b2')\n",
    "    net._fc = nn.Linear(in_features=1408, out_features=4, bias=True)\n",
    "    return net\n",
    "\n",
    "mx2 = get_net2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = \"../input/alaska2-image-steganalysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting validation data and labels\n",
    "# %%time\n",
    "data = []\n",
    "for label, kind in enumerate([\"Cover\", \"JMiPOD\", \"JUNIWARD\", \"UERD\"]):\n",
    "    for i, path in enumerate(glob(\"../input/alaska2-image-steganalysis/Cover/*.jpg\")):\n",
    "#         if i == 2500:\n",
    "#             break\n",
    "        dat = {\n",
    "            \"kind\": kind,\n",
    "            \"image_name\": path.split(\"/\")[-1],\n",
    "            \"label\": label\n",
    "        }\n",
    "        data.append(dat)\n",
    "data = pd.DataFrame(data).sample(60000, random_state = 42)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(height = 512, width = 512, p = 1.0),\n",
    "        A.HorizontalFlip(p = 0.5),\n",
    "        A.VerticalFlip(p = 0.5),\n",
    "        ToTensorV2(p = 1.0)\n",
    "    ], p = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatasetEnsembleRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, image_names, image_kinds, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_names = image_names\n",
    "        self.image_kinds = image_kinds\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_name = self.image_names[index]\n",
    "        image_kind = self.image_kinds[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_kind}/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "\n",
    "        return image_name, image\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]\n",
    "    \n",
    "    def get_names(self):\n",
    "        return self.image_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetEnsembleRetriever(\n",
    "    image_names=data[\"image_name\"].values,\n",
    "    image_kinds = data[\"kind\"].values,\n",
    "    transforms=get_valid_transforms(),\n",
    ")\n",
    "\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model2 = mx2.to(device)\n",
    "# model0 = mx0.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating checkpoint for all gpu models\n",
    "\n",
    "####################################    (Efficientnet b2)     #########################################\n",
    "# Alex\n",
    "cp1 = torch.load(\"../input/alaska2-public-baseline/best-checkpoint-023epoch.bin\")\n",
    "cp2 = torch.load(f\"../input/alaska2-public-baseline/best-checkpoint-033epoch.bin\")\n",
    "\n",
    "# # Sid\n",
    "\n",
    "# cp3 = torch.load(\"../input/sid-epochs88/epoch_0_val_loss_6.07_auc_0.867.pth\")\n",
    "# cp4 = torch.load(\"../input/sid-epochs88/epoch_1_val_loss_6.12_auc_0.864.pth\")\n",
    "\n",
    "# # ####################################    (Efficientnet b0)    ##########################################\n",
    "\n",
    "# cp5 = torch.load(\"../input/alaska2-efficientnet-trained-model-weights/efficientnetb0_lb0.867.pth\")\n",
    "# cp6 = torch.load(\"../input/alaska2-efficientnet-trained-model-weights/efficientnetb0_lb0.870.pth\")\n",
    "# cp7 = torch.load(\"../input/alaska2-efficientnet-trained-model-weights/efficientnetb0_lb0.871.pth\")\n",
    "# cp8 = torch.load(\"../input/alaska2-efficientnet-trained-model-weights/efficientnetb0_lb0.876.pth\")\n",
    "# cp9 = torch.load(\"../input/alaska2-efficientnet-trained-model-weights/efficientnetb0_lb0.881.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "=========================\n",
      "=========================\n",
      "=========================\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# Alex\n",
    "for i in range(5):\n",
    "    for cp in [cp1, cp2]:\n",
    "        model2.load_state_dict(cp['model_state_dict'])\n",
    "        result = []\n",
    "        for step, (image_names, images) in enumerate(data_loader):\n",
    "            y_pred = model2(images.cuda())\n",
    "            y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "            result.extend(y_pred)\n",
    "        results.append(result)\n",
    "        \n",
    "    print(\"=\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = pd.DataFrame(results).T\n",
    "valid_data[\"label\"] = data[\"label\"].values\n",
    "valid_data.to_csv(\"ensemble_req.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transforms():\n",
    "    return A.Compose([\n",
    "        A.Resize(height = 512, width = 512, p = 1.0),\n",
    "        A.HorizontalFlip(p = 0.5),\n",
    "        A.VerticalFlip(p = 0.5),\n",
    "        ToTensorV2(p = 1.0)\n",
    "    ], p = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b2\n"
     ]
    }
   ],
   "source": [
    "def get_net():\n",
    "    net = EfficientNet.from_pretrained('efficientnet-b2')\n",
    "    net._fc = nn.Linear(in_features=1408, out_features=4, bias=True)\n",
    "    return net\n",
    "\n",
    "mx = get_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSubmissionRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, image_names, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_names = image_names\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_name = self.image_names[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/Test/{image_name}', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "\n",
    "        return image_name, image\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_names.shape[0]\n",
    "    \n",
    "    def get_names(self):\n",
    "        return self.image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetSubmissionRetriever(\n",
    "    image_names=np.array([path.split('/')[-1] for path in glob('../input/alaska2-image-steganalysis/Test/*.jpg')]),\n",
    "    transforms=test_transforms(),\n",
    ")\n",
    "\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "=========================\n",
      "=========================\n",
      "=========================\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_results = []\n",
    "for i in range(5):\n",
    "    for cp in [cp1, cp2]:\n",
    "        model2.load_state_dict(cp['model_state_dict'])\n",
    "        result = []\n",
    "        for step, (image_names, images) in enumerate(data_loader):\n",
    "            y_pred = model2(images.cuda())\n",
    "            y_pred = 1 - nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,0]\n",
    "            result.extend(y_pred)\n",
    "        test_results.append(result)\n",
    "        \n",
    "    print(\"=\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame(test_results).T\n",
    "test_data[\"Id\"] = list(dataset.get_names())\n",
    "test_data.to_csv(\"test_req.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "11c0a8d7fa6646fa84a52ab9e85d5584": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_19b1900967704798b3b7ca090f2bf418",
        "IPY_MODEL_6ef5f49426604107a4f3f1c0e30ae0d2"
       ],
       "layout": "IPY_MODEL_9c37e272ac7647efb45ec7b6f2517206"
      }
     },
     "19b1900967704798b3b7ca090f2bf418": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_68f1f8b44ec64a25945ccb04df871ec3",
       "max": 36804509.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_31cadab7994c44898df9bb856077e877",
       "value": 36804509.0
      }
     },
     "31cadab7994c44898df9bb856077e877": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "68f1f8b44ec64a25945ccb04df871ec3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ef5f49426604107a4f3f1c0e30ae0d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b8e09ddbfb054bd581c1509ffaec2da7",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_7f428ef576e14130b5a9c2585be8884e",
       "value": " 35.1M/35.1M [00:03&lt;00:00, 10.6MB/s]"
      }
     },
     "7f428ef576e14130b5a9c2585be8884e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9c37e272ac7647efb45ec7b6f2517206": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8e09ddbfb054bd581c1509ffaec2da7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
